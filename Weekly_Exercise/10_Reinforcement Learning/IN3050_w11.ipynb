{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3050/4050 - Week 11: Reinforcement Learning\n",
    "\n",
    "**Reinforcement Learning** (RL) is the last machine learning paradigm covered in this course. We will only take a cursory look at it. There is a lot of depth to this topic (just as for *supervised* and *unsupervised* learning), and we can of course not cover it in one single week. For those that are interested we can recomend the book \"Reinforcement Learning: An Introduction\" by Richard S. Sutton and Andrew G. Barto.\n",
    "\n",
    "In these exercises we will focus on understanding the main concepts, and implementing basic reinforcement learning scenarios and solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "Let us first redefine the main concepts underlying reinforcement learning:\n",
    "1. What is an agent?\n",
    "2. What is an environment?\n",
    "3. What is a state?\n",
    "4. What is an action?\n",
    "5. What is a policy?\n",
    "6. What is a reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "1. **Agent**: An entity interacting with an enviornment according to a policy. \n",
    "\n",
    "2. **Enviroment**: The entity with wich the agent interact, and from which the agent receives feedbacks and reward. \n",
    "\n",
    "3. **State**: A descriptor of the environment and the agent (a sfficicent statistics of the history for teh agents decision). \n",
    "\n",
    "4. **Action**: A way for the agent to interact with the eenvionment. \n",
    "\n",
    "5. **Policy**: A rule to choose actions given states. \n",
    "\n",
    "6. **Reward**: A signal denoting how good the current action and, implicitily, the preceding ones where.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this section we are going to run some simulations on a simple *GridWorld* environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We first import some support libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import constants as const\n",
    "import policies as plcs\n",
    "import worlds as wlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **constants** simply contains some handy aliases. Actions and objects in the environment are identified by integer numbers. This module allows you to refer to actions using the format *const.LEFT* instead of an integer number; similarly objects in the world can be referred to *const.WALL* instead of an integer number. Check out the source file if you want to see the association between aliases and their integer encoding.\n",
    "\n",
    "The module **policies** contains a set of pre-made policies that we will use in the exercise.\n",
    "\n",
    "The module **worlds** contains a set of pre-made gridworlds that we will use in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: GridWorld\n",
    "Our environment is a simple GridWorld, that is a discrete flat rectangular environment made up $N\\times M$ squares, thus defining a grid. One square is identified as the *starting position*, and another square is the *ending position*. An agent start in the starting position, and its aim is to reach the ending position. A gridworld may be enriched with additional *walls* (that is, squares where the agent can not pass) and *traps* (that is, squares that kill the agent and terminate the episode).\n",
    "\n",
    "This gridworld model is **completely deterministic** (actions never fail) and **completely observed** (the agent perfectly knows the state of the world). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gridworld as gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **gridworld** contains the class *GridWorld* implementing the environment with which the agent interact. An object *GridWorld* offers the following interface:\n",
    "- **GridWorld(width,height, staring_position_x,staring_position_y, ending_position_x,ending_position_y)**: this is the constructor for a gridworld. It expects six parameters: the *width* and the *length* of the gridworld; the coordinates x and y of the starting position (*staring_position_x,staring_position_y*) and of the ending position (ending_position_x,ending_position_y). It returns an instance of a gridworld.\n",
    "- **add_wall(x,y)**: this is a function to add a wall to the gridworld in location *x,y*.\n",
    "- **add_trap(x,y)**: this is a function to add a trap to the gridworld in location *x,y*.\n",
    "- **step(action)**: this is a function that takes an *action* passed as a parameter. The gridworld processes the action and returns four values: the new *state* of the world (that is, the new position of the agent in the form of a Loc object with two attributes x and y); the *reward* obtained by the agent by performing the action; a boolean *termination* indicating whether the episode terminated; and, optionally, some debug messages. \n",
    "- **reset()**: this is a function to reset the environment to the starting condition; it returns the same values as step().\n",
    "- **print_basic()**: this is a function to produce a simple textual representation of the environment on the screen.\n",
    "- **print_unicode()**: this a slightly fancier printing function using Unicode characters (if support is available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAUTION**: A gridworld is indexed as a numpy matrix. Thus, if you build a $10 \\times 10$ matrix, the height and the width of the gridworld will be 10; however the square in the world will be indexed from 0 to 9, with (0,0) being the upper-left corner square, and (9,9) being the lower-right corner square. \n",
    "\n",
    "\n",
    "An example $3 \\times 3$ world:\n",
    "![title](grid.png)\n",
    "\n",
    "**Verification of consistency in *YOUR* hands!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create now a $4 \\times 4$ gridworld with starting position in the upper-left corner and ending position in botttom-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gw.GridWorld(4,4,0,0,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add a wall near the center in location (1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.add_wall(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the world using one of the supported print functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1    0    0    0 ]\n",
      " [  0    0    0    0 ]\n",
      " [  0    0    0    0 ]\n",
      " [  0    0    0    8 ]]\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W.print_basic()\n",
    "W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the basic rendering 0 is an open space, 1 is the starting position, 6 is a trap, 7 is a wall, 8 is the agent and 9 is the ending position.\n",
    "\n",
    "In the unicode rendering a white square is an open space, alpha is the starting position, a cross is a trap, a black square is a wall, the agent is a smile and omega is the ending position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting is a useful operation to restart the environment clean. It is a good practice to call it before starting any simulation to be certain that the environment is in the starting state. Call the reset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, termination, msgs = W.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the action of a step to the right, print the return values of the action, visualize the world again, and comment on the action and its consequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gridworld.Loc object at 0x1054327e0>, -1, False, ['I moved', None, None])\n",
      "Α ☺ □ □\n",
      "□ □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(W.step(const.RIGHT))\n",
    "W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Legit move, we get closer to the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try now to take a step in the up direction, print the return values of the action, visualize, and comment on the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gridworld.Loc object at 0x1054327e0>, -1, False, ['I can not move out of the world', None, None])\n",
      "Α ☺ □ □\n",
      "□ □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(W.step(const.UP))\n",
    "W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see that we cant move out of the world and get a penalty for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lead the agent to the objective, print the return values of the action, visualize, and comment on the actions and the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gridworld.Loc object at 0x1054c4680>, -1, True, 'Game is over')\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "(<gridworld.Loc object at 0x1054c4680>, -1, True, 'Game is over')\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "(<gridworld.Loc object at 0x1054c4680>, -1, True, 'Game is over')\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "(<gridworld.Loc object at 0x1054c4680>, -1, True, 'Game is over')\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "(<gridworld.Loc object at 0x1054c4680>, -1, True, 'Game is over')\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(W.step(const.DOWN))\n",
    "W.print_unicode()\n",
    "print(W.step(const.DOWN))\n",
    "W.print_unicode()\n",
    "print(W.step(const.RIGHT))\n",
    "W.print_unicode()\n",
    "print(W.step(const.DOWN))\n",
    "W.print_unicode()\n",
    "print(W.step(const.RIGHT))\n",
    "W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see for each move we get -1 and when we reach the goal we get 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the world to the starting state, print the return values of the action, visualize, and comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset the world\n",
    "state, reward, termination, msgs = W.reset()\n",
    "W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We are now at the start. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "One of the main problems in reinforcement learning is **policy evaluation**: *given a policy for acting in the environment, how good is the policy?*\n",
    "\n",
    "We will consider and answer this question first for simple *deterministic* policies (computing the simple overall return of a policy) and then for *stochastic* policies (using Monte Carlo to estimate the average overall return of a policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "First load *world1()* from *wrlds* and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = wlds.world1()\n",
    "W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a deterministic policy\n",
    "We will first use the **deterministic policy** *plcs.policy1()*. This defines a policy that moves the agent always to the right until it reaches the border, and then heads down.\n",
    "\n",
    "Design a loop that makes the agent interact with the environment using this policy until termination. Visualize the output at each step to verify that the policy works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state, reward, termination, msgs = W.reset() \n",
    "\n",
    "while not termination:\n",
    "    action = plcs.policy1(state)\n",
    "    state,reward, termination,msgs = W.step(action)\n",
    "    W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the previous loop to track the states traversed, the actions taken, and the rewards collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "state,reward, termination,msgs = W.reset()\n",
    "states.append(state)\n",
    "\n",
    "while not termination:\n",
    "    action = plcs.policy1(state)\n",
    "    actions.append(action)\n",
    "    state,reward, termination,msgs = W.step(action)\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the return of a deterministic policy\n",
    "One of the main quantities to evaluate the goodness of a policy is the *return*. While a *reward* quantifies the local/short-term goodness of a single action, the *return* quantifies the global/long-term goodness of a policy.\n",
    "\n",
    "Remember that we can compute the overall return of a policy $\\pi$ from time $t$ using a discount $\\gamma$ as:\n",
    "\n",
    "$$\n",
    "G_{\\pi}(t) = r(t) + \\sum_i \\gamma^i r(t+i),\n",
    "$$\n",
    "where $G_{\\pi}(t)$ is the return at time $t$ and r(t) is the reward at time $t$ under policy ${\\pi}$. \n",
    "\n",
    "Write code to compute the return of policy *plcs.policy1()* from the initial state at $t=0$ with a discount rate of $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: 54.953900000000004\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "G = 0\n",
    "\n",
    "for i in range(len(rewards)):\n",
    "    G += rewards[i] * (gamma ** i)\n",
    "\n",
    "print('Return: {0}'.format(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run now the same code using the policy *plcs.policy2()*. This defines a deterministic policy that moves the agent first downwards, then upwards, and finally right and down to the objective. Plot the policy, compute its return and comment on the two policies you analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "Return: 24.519165569900004\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists for new trajectory\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "# Reset the environment\n",
    "state, reward, termination, msgs = W.reset()\n",
    "\n",
    "# Add initial state\n",
    "states.append(state)\n",
    "\n",
    "# Run episode with policy2\n",
    "while not termination:\n",
    "    # Get action from policy2\n",
    "    action = plcs.policy2(state)\n",
    "    \n",
    "    # Store the action\n",
    "    actions.append(action)\n",
    "    \n",
    "    # Take the action\n",
    "    state, reward, termination, msgs = W.step(action)\n",
    "    \n",
    "    # Store state and reward\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # Visualize\n",
    "    W.print_unicode()\n",
    "\n",
    "# Compute return with γ = 0.9\n",
    "gamma = 0.9\n",
    "G = 0\n",
    "for i in range(len(rewards)):\n",
    "    G += rewards[i] * (gamma ** i)\n",
    "\n",
    "print('Return: {0}'.format(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see policy 1 is more direct since we end up with 54 compared to policy2 where we end up with 24. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of the deterministic policy\n",
    "The deterministic policy *plcs.policy1()* is optimal for *wlds.world1()*. However deterministic policies have severe limitations.\n",
    "\n",
    "Load *world2* from *wlds*, plot the environment, and run 10 steps of *plcs.policy1()*. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☺ □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = wlds.world2()\n",
    "W.print_unicode()\n",
    "\n",
    "for i in range(10):\n",
    "    action = plcs.policy1(state)\n",
    "    state,reward, termination,msgs = W.step(action)\n",
    "    W.print_unicode()\n",
    "\n",
    "    if termination:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see that we find the goal profecient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a stochastic policy\n",
    "\n",
    "Having assessed the limitation of a deterministic policy, we now consider a **stochastic policy**. A stochastic policy adds some randomness in the selection of the action to perform. Taking random actions will allow the agent to *explore* more of the world and will provide a higher degree of *generalization*/*adaptability*.\n",
    "We use here *plcs.policy_epsilon1()*, a stochastic variant of *plcs.policy1()*, that behaves according to *plcs.policy1()*, except that 20% of the time it will take a random action.\n",
    "\n",
    "Run *plcs.policy1()* both on *world1* and *world2* and comment on the results. (You may want to run your simulation more than one time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = wlds.world1()\n",
    "W.print_unicode()\n",
    "\n",
    "state, reward, termination, msgs = W.reset()\n",
    "while not termination:\n",
    "    action = plcs.policy_epsilon1(state)\n",
    "    state,reward,termination,msgs = W.step(action)\n",
    "    W.print_unicode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☺ □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ■ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = wlds.world2()\n",
    "W.print_unicode()\n",
    "\n",
    "state, reward, termination, msgs = W.reset()\n",
    "while not termination:\n",
    "    action = plcs.policy_epsilon1(state)\n",
    "    state,reward,termination,msgs = W.step(action)\n",
    "    W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see that the world1 is and world2 is more random now and in world2 we are stuck longer before moving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the return of a stochastic policy\n",
    "As in the case of deterministic policies, we want to be able to quantify the goodness of a stochastic policy. However, the simple approach we adopted above will not immediately work in the case of a stochastic policy. Any two simulations using a stochastic policy will likely return different results, and computing returns on each of them will yield different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Policy Evaluation (return)\n",
    "\n",
    "As discussed above, we can not evaluate the return of a stochastic policy deterministically from a single simulation. Indeed, two different simulations would yield different returns. An intuitive solution to this problem would be to compute the return of the stochastic policy *in expectation*.\n",
    "\n",
    "**Monte Carlo methods** constitute a wide family of techniques based on simulation and estimation of expected quantities. Imagine you are given a system whose dynamics are unknown or too hard to analyze analytically; suppose also that the system can be cheaply simulated; then we can try to answer questions about this system by repeatedly running simulations, collecting samples, and computing statistical quantities about the system.\n",
    "\n",
    "In our case, the dynamics of the environment (how we transition from one state to another given an action) are known and we could theoretically compute analytically the value of the policy. However we assume that this computation is too expensive and we rely on a Monte Carlo approximation. \n",
    "\n",
    "What we (and the agent) do is trying out a given policy many times; each repetition, from the starting state to termination, is called an *episode*; the list of actions, states and rewards collected during an episode is called a *trajectory*. Given a set of trajectories, we can compute the expected value of our policy.\n",
    "\n",
    "As you know, *Monte-Carlo* is just one approach to (tabular) policy evaluation in the case of stochastic policies. Alternatives are *dynamic programming* (where we assume the agent knows the dynamics of the environment) or *temporal differences* (where we assume that we do not have complete episodes).\n",
    "\n",
    "Load *wlds.world1()* and *plcs.policy_epsilon1()*, and run 50 episodes with this setting while collecting the trajectories of your simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 50\n",
    "episodes = []\n",
    "\n",
    "# Run 50 episodes\n",
    "for i in range(n_episodes):\n",
    "    # Initialize lists for this episode's trajectory\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset environment\n",
    "    state, reward, termination, msgs = W.reset()\n",
    "    \n",
    "    # Add initial state\n",
    "    states.append(state)\n",
    "    \n",
    "    # Run episode until termination\n",
    "    while not termination:\n",
    "        # Get action from stochastic policy\n",
    "        action = plcs.policy_epsilon1(state)\n",
    "        \n",
    "        # Store action\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Take action\n",
    "        state, reward, termination, msgs = W.step(action)\n",
    "        \n",
    "        # Store state and reward (ensure reward is a number)\n",
    "        states.append(state)\n",
    "        rewards.append(float(reward))  # Convert reward to float when storing\n",
    "    \n",
    "    # Store this episode's trajectory\n",
    "    episodes.append({\n",
    "        'states': states,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards  # Now contains only float values\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the data we have collected to compute the return in expectation:\n",
    "$$ G_{\\pi}(t) = \\mathbb{E} \\left[r(t) + \\sum_i \\gamma^i r(t+i)\\right]. $$\n",
    "\n",
    "This simply corresponds to the average return over the episodes:\n",
    "$$ \\hat{G}_{\\pi}(t) = \\frac{1}{N_e} \\sum_{e=1}^{N_e} \\left( r^{(e)}(t) + \\sum_i \\gamma^i r^{(e)}(t+i) \\right), $$\n",
    "where $r^{(e)}$ is the reward obtained during episode number $e$, and $N_e$ is the number of episodes collected.\n",
    "\n",
    "Compute the expected return of policy *plcs.policy_epsilon1()* from the trajectories you collected, compare it with the return of the deterministic policy and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: 10.147183657386634\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9  # Discount factor - future rewards are worth 90% of immediate rewards\n",
    "totG = 0.0   # Initialize total return across all episodes\n",
    "\n",
    "# Loop through each of the 50 episodes\n",
    "for i in range(n_episodes):\n",
    "    # Get the list of rewards from this episode's trajectory\n",
    "    episode_rewards = episodes[i]['rewards']\n",
    "    \n",
    "    # Calculate the discounted return for this specific episode\n",
    "    G = 0\n",
    "    for t in range(len(episode_rewards)):\n",
    "        # At each timestep t:\n",
    "        # - episode_rewards[t] is the reward at that time\n",
    "        # - gamma ** t is the discount (0.9^t)\n",
    "        # - Multiply them and add to episode return\n",
    "        G += episode_rewards[t] * (gamma ** t)\n",
    "    \n",
    "    # Add this episode's return to the running total\n",
    "    totG += G\n",
    "\n",
    "# Calculate the average return across all episodes\n",
    "hatG = totG / n_episodes   \n",
    "print('Return: {0}'.format(hatG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement\n",
    "\n",
    "So far, we have only considered the problem of evaluating how good a given policy is. We now consider a second central problem in reinforcement learning, that is, **policy improvement**: *given a policy, how can we construct a better policy?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State values and Action-state values\n",
    "In the previous section we have evaluated policies by computing or estimating their overall return. This is a **global** value that assesses the goodness of a predefined policy from the beginning to the end. Comparing two policies according to their overall return is simple. But improving policies relying only on this quantity is tricky. Generating new policies, computing their overall return, and comparing with existing policies is unfeasible, as the number of possible policies explodes with the number of states and actions.\n",
    "\n",
    "Ideally, we need some form of **local** information that would allow us to assemble new policies in a *modular* way. These quantities are:\n",
    "- **state values** $v(s)$: evaluating how good is it to be in a certain state $s$ (more precisely, what is the expected return for a certain state $s$).\n",
    "- **action-state values** $q(s,a)$: evaluating how good is it to take a certain action $a$ being in a certain state $s$ (more precisely, what is the expected return from a certain state $s$ by taking action $a$).\n",
    "By knowing locally how good is a certain state or how good is a certain action we can steer the choices towards better policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State values\n",
    "\n",
    "The state value $v(s)$ of a state $s$ under the policy $\\pi$ is defined as:\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E} \\left[ G(t) \\vert S(t)=s \\right],\n",
    "$$\n",
    "that is the return, as we computed above, but starting now from state $s$ instead of the initial state.\n",
    "\n",
    "As before, computing the return for a stochastic policy is not immediate, and it requires relying on one of the approaches referenced above (*dynamic programming, Monte Carlo, temporal differences*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Policy Evaluation for State Values\n",
    "\n",
    "The same Monte Carlo principle we used to estimate overall returns may be used to compute state values: we run several episodes of the agents acting in the world and then we compute the state values.\n",
    "\n",
    "First we generate the data. We use the same environment, but we now consider as a starting policy a completely random policy. This choice represents our complete ignorance about the environment (no one provided us with an almost optimal policy) and it will allow the agent to explore the environment more widely.\n",
    "\n",
    "Load *wlds.world1()* and *plcs.policy_random()*, and run 100 episodes with this setting while collecting the trajectories of your simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "episodes = []\n",
    "\n",
    "# Load world1\n",
    "W = wlds.world1()\n",
    "\n",
    "# Run 100 episodes with random policy\n",
    "for i in range(n_episodes):\n",
    "    # Initialize lists for this episode's trajectory\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset environment\n",
    "    state, reward, termination, msgs = W.reset()\n",
    "    \n",
    "    # Add initial state\n",
    "    states.append(state)\n",
    "    \n",
    "    # Run episode until termination\n",
    "    while not termination:\n",
    "        # Get action from random policy\n",
    "        action = plcs.policy_random(state)\n",
    "        \n",
    "        # Store action\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Take action\n",
    "        state, reward, termination, msgs = W.step(action)\n",
    "        \n",
    "        # Store state and reward (ensure reward is a float)\n",
    "        states.append(state)\n",
    "        rewards.append(float(reward))\n",
    "    \n",
    "    # Store this episode's trajectory\n",
    "    episodes.append({\n",
    "        'states': states,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the state value function $v(s)$ of each state $s$ using the *first-visit* Monte Carlo algorithm. For each state $s$, we find the first instance in an episode and compute its return. At the end we compute the expected return.\n",
    "\n",
    "Compute the state value function using the first-visit Monte Carlo algorithm. \n",
    "1. Iterate over every state; a state is encoded as *gw.Loc()* object from the module *gw*; the constructor *gw.Loc(x,y)* receives the $x,y$ coordinates of a square in the grid world.\n",
    "2. Iterate over the episodes; in each episode find the first occurence of the current state; you can use the function *gw.find_location_in_array()* from the module *gw*. The function *gw.find_location_in_array(loc, list)* receives a location *loc* in format *gw.Loc()* and a list of locations; it returns the index of the first instance of *loc* within *list*, or *None* if *loc* does not appear in *list*.\n",
    "3. Compute the return from the first occurrence of the current state.\n",
    "4. Average the return over all the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = {}  # Dictionary to store state values\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Iterate over all possible states in 4x4 grid\n",
    "for x in range(4):\n",
    "    for y in range(4):\n",
    "        s = gw.Loc(x,y)  # Create state location\n",
    "        Gs = []  # List to store returns for this state\n",
    "        \n",
    "        # Iterate over all episodes\n",
    "        for episode in episodes:\n",
    "            # Find first occurrence of state s in this episode\n",
    "            first_visit_idx = gw.find_location_in_array(s, episode['states'])\n",
    "            \n",
    "            # If state was visited in this episode\n",
    "            if first_visit_idx is not None:\n",
    "                # Calculate return from first visit onwards\n",
    "                G = 0\n",
    "                rewards = episode['rewards'][first_visit_idx:]  # Get rewards from first visit\n",
    "                \n",
    "                # Calculate discounted return\n",
    "                for t in range(len(rewards)):\n",
    "                    G += rewards[t] * (gamma ** t)\n",
    "                \n",
    "                # Add this return to list of returns for state s\n",
    "                Gs.append(G)\n",
    "        \n",
    "        # Average returns for this state if we have any visits\n",
    "        if Gs:\n",
    "            Vs[s] = np.mean(Gs)\n",
    "        else:\n",
    "            Vs[s] = 0  # If state never visited, set value to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now place the state-values in a matrix V (respecting the convention discussed at the beginning on the labelling of the squares), print the matrix, and comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.01244966e+00  3.35724787e-03  2.29575557e+00  6.92653852e+00]\n",
      " [ 1.19151356e+00  4.08934963e+00  1.06043939e+01  1.90127456e+01]\n",
      " [ 2.12600005e+00  8.44517031e+00  2.47479133e+01  4.01773654e+01]\n",
      " [ 4.11599180e+00  1.42951578e+01  4.01484889e+01  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "V = np.zeros((4,4))\n",
    "for x in range(4):\n",
    "    for y in range (4):\n",
    "        state = gw.Loc(x,y)\n",
    "        V[x,y] = Vs[state] if state in Vs else 0\n",
    "        \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The matrix shows a gradient toward the terminal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "Our agent started with a purely random policy. Now that we have computed the state values of our grid world we can select an optimal policy. A simple way to use the state-value matrix to improve on our random policy is the following one: at each state select the action that leads you to the successor state with highest value.\n",
    "\n",
    "Define a policy (a function receiving a state location as an input) using the state-value matrix you computed above to define an improved policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_policy(state):\n",
    "    value_next_state = {}\n",
    "    \n",
    "    if(state.x>0):\n",
    "        value_next_state[const.UP] = V[state.x-1,state.y]\n",
    "    else:\n",
    "        value_next_state[const.UP] = -np.inf\n",
    "    # Check DONW move if not at bottom edge\n",
    "    if(state.x < 3):\n",
    "        value_next_state[const.DOWN] = V[state.x+1,state.y]\n",
    "    else:\n",
    "        value_next_state[const.DOWN] = -np.inf\n",
    "\n",
    "    #Check LEFT move if not at left edge\n",
    "    if(state.y >0):\n",
    "        value_next_state[const.LEFT] = V[state.x, state.y-1]\n",
    "    else:\n",
    "        value_next_state[const.LEFT] = -np.inf\n",
    "\n",
    "    # Check RIGHT move if not at right edge\n",
    "    if(state.y < 3):\n",
    "        value_next_state[const.RIGHT] = V[state.x,state.y+1]\n",
    "    else:\n",
    "        value_next_state[const.RIGHT] = -np.inf\n",
    "        \n",
    "    return max(value_next_state,key=value_next_state.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of policy is called a *greedy policy* because, at each step, it always deterministically takes the action that is expected to produce the higher return.\n",
    "\n",
    "Load *wlds.world1()* and use the greedy policy you have just defined to control the agent. Plot the outcome, and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: At position (0, 0), taking action 2\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 1: At position (1, 0), taking action 3\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 2: At position (1, 1), taking action 3\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 3: At position (1, 2), taking action 2\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 4: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 5: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 6: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 7: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 8: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 9: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 10: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 11: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 12: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 13: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 14: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 15: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 16: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 17: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 18: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 19: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 20: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 21: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 22: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 23: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 24: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 25: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 26: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 27: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 28: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 29: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 30: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 31: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 32: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 33: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 34: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 35: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 36: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 37: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 38: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 39: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 40: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 41: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 42: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 43: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 44: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 45: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 46: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 47: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 48: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 49: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 50: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 51: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 52: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 53: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 54: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 55: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 56: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 57: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 58: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 59: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 60: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 61: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 62: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 63: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 64: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 65: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 66: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 67: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 68: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 69: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 70: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 71: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 72: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 73: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 74: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 75: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 76: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 77: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 78: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 79: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 80: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 81: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 82: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 83: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 84: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 85: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 86: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 87: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 88: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 89: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 90: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 91: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 92: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 93: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 94: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 95: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 96: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 97: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 98: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (2, 3), taking action 0\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Episode terminated due to maximum steps reached\n"
     ]
    }
   ],
   "source": [
    "# Load world1\n",
    "W = wlds.world1()\n",
    "\n",
    "# Reset environment\n",
    "state, reward, termination, msgs = W.reset()\n",
    "\n",
    "max_steps = 100\n",
    "step_count = 0\n",
    "\n",
    "# Run episode with our greedy policy\n",
    "while not termination and step_count < max_steps:\n",
    "    # Get action from our greedy policy\n",
    "    action = my_policy(state)\n",
    "    print(f\"Step {step_count}: At position ({state.x}, {state.y}), taking action {action}\")\n",
    "    # Take action\n",
    "    state, reward, termination, msgs = W.step(action)\n",
    "    \n",
    "    # Visualize current state\n",
    "    W.print_unicode()\n",
    "    step_count += 1\n",
    "\n",
    "if step_count >= max_steps:\n",
    "    print(\"Episode terminated due to maximum steps reached\")\n",
    "else:\n",
    "    print(\"Episode terminated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: It uses a lot of time to find the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A completely deterministic policy as the one we defined is always susceptible to underperform if the environment undergoes any change.\n",
    "\n",
    "Design a world, possibly with minimal changes from *wlds.world1()*, where the optimal policy just learned fails. Run this world for 10 iterations using the learned greedy policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 99: At position (0, 0), taking action 2\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (1, 0), taking action 3\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (1, 1), taking action 3\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (1, 2), taking action 2\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Step 99: At position (2, 2), taking action 3\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ ■\n",
      "□ □ □ Ω\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = gw.GridWorld(4,4, 0,0, 3,3)\n",
    "W.add_wall(2,3)\n",
    "\n",
    "state, reward, termination, msgs = W.reset()\n",
    "for _ in range(10):\n",
    "    action = my_policy(state)\n",
    "    print(f\"Step {i}: At position ({state.x}, {state.y}), taking action {action}\")\n",
    "    state,reward,termination,msgs = W.step(action)\n",
    "\n",
    "    W.print_unicode()\n",
    "\n",
    "    if termination:\n",
    "        print(\"Episode terminated\")\n",
    "        break\n",
    "\n",
    "if i == 9:\n",
    "    print(\"Maximum steps reached without finding goal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, while learning and acting in the environment, it is a good idea to preserve some degree of *stochasticity* to keep analyzing the world and improve your policy.\n",
    "\n",
    "Improving the policy of an agent is a central problem in reinforcement learning. The problem of **control** consists in finding or approximating an optimal policy. A common approach, which we have followed in this notebook, consists in alternating a step of *policy evaluation* and a step of *policy improvement*. In the example above, we have started with a random policy and we have computed the associated state-value matrix (policy evaluation); then, we have defined a greedy policy using the state-value matrix (policy improvement). This generic learning approach is called **Generalized Policy Iteration**. On more complex problem, several iterations of evaluation-improvement may be required to approximate an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Q-Learning\n",
    "\n",
    "**Q-Learning** is an *off-policy*, *temporal difference* reinforcement learning algorithm. \n",
    "Off-policy means that the agent learns an optimal target policy $\\pi_t$ while behaving according to a different behaviour policy $\\pi_b$; this allows, for instance, the agent to behave according to a sub-optimal exploratory policy while learning and memorizing an optimal policy. Temporal difference means that the agent uses an algorithm that allows learning in real-time without the need to collect whole episodes (as it was in the case of Monte Carlo).\n",
    "\n",
    "Q-Learning does not rely on any model of the environment, and as such, it works estimating *state-action values* $q(s,a)$ instead of *state values* $v(s)$.\n",
    "\n",
    "First, load *wlds.world1()* and randomly initialize a matrix/tensor to store the state-action values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wlds.world1()\n",
    "Q = np.random.uniform(low=0, high=0.1, size=(4, 4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While acting in the world, we are going to define as a *behaviour policy* an $\\epsilon$-greedy policy that, for each state, takes a random action with probability $\\epsilon$ or the optimal action defined by the state-action matrix $q(s,a)$ with probability $1-\\epsilon$.\n",
    "\n",
    "Implement this policy as a function receiving as input the current state in the form of *gw.Loc()*, the current state-action matrix $q(s,a)$, and the parameter $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behaviour_policy(state,Q,eps):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.choice([const.UP,const.DOWN,const.LEFT,const.RIGHT])\n",
    "    else:\n",
    "        q_values = Q[state.x, state.y, :]\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our learning with the following parameters: discount factor $\\gamma = 0.9$, learning rate $\\alpha=0.1$, exploration stochasticity $\\epsilon = 0.4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement and run Q-learning for one episode.\n",
    "\n",
    "1. Given that we are in state $s$, we choose an action $a$ according to our behaviour policy *behaviour_policy()*\n",
    "2. We take action $a$, receive reward $r$ and end up in state $s'$\n",
    "3. We update the action-value function $q(s,a)$ according to the following formula:\n",
    "$$\n",
    "q(s,a) \\leftarrow q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} q(s',a') - q(s,a) \\right]\n",
    "$$\n",
    "where $\\max_{a'} q(s',a')$ is the action-value of the action $a'$ that maximizes the action-value for state $s'$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset environment\n",
    "state, reward, termination, msgs = W.reset()\n",
    "\n",
    "# Run one episode\n",
    "while not termination:\n",
    "    # 1. Choose action using behavior policy (ε-greedy)\n",
    "    action = behaviour_policy(state, Q, epsilon)\n",
    "    \n",
    "    # 2. Take action, get new state and reward\n",
    "    next_state, reward, termination, msgs = W.step(action)\n",
    "    \n",
    "    # 3. Q-learning update\n",
    "    # Current Q-value\n",
    "    current_q = Q[state.x, state.y, action]\n",
    "    \n",
    "    # Max Q-value for next state\n",
    "    next_max_q = np.max(Q[next_state.x, next_state.y, :]) if not termination else 0\n",
    "    \n",
    "    # Q-learning update formula\n",
    "    Q[state.x, state.y, action] = current_q + alpha * (reward + gamma * next_max_q - current_q)\n",
    "    \n",
    "    # Move to next state\n",
    "    state = next_state\n",
    "    \n",
    "    # Visualize current state\n",
    "    W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the values in the action-value matrix, for instance in the pre-final states (2,3) or (3,2). Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for state (2,3):\n",
      "UP: 0.09316324640008916\n",
      "DOWN: 0.08319644062058529\n",
      "LEFT: 10.088350297937565\n",
      "RIGHT: 0.060055683335476895\n",
      "\n",
      "Q-values for state (3,2):\n",
      "UP: 0.01981638408356259\n",
      "DOWN: 0.03659605625347511\n",
      "LEFT: 0.07246293084005005\n",
      "RIGHT: 0.09050556937512302\n"
     ]
    }
   ],
   "source": [
    "# Print Q-values for state (2,3)\n",
    "print(\"Q-values for state (2,3):\")\n",
    "print(\"UP:\", Q[2,3,0])    # UP action\n",
    "print(\"DOWN:\", Q[2,3,1])  # DOWN action\n",
    "print(\"LEFT:\", Q[2,3,2])  # LEFT action\n",
    "print(\"RIGHT:\", Q[2,3,3]) # RIGHT action\n",
    "\n",
    "print(\"\\nQ-values for state (3,2):\")\n",
    "print(\"UP:\", Q[3,2,0])    # UP action\n",
    "print(\"DOWN:\", Q[3,2,1])  # DOWN action\n",
    "print(\"LEFT:\", Q[3,2,2])  # LEFT action\n",
    "print(\"RIGHT:\", Q[3,2,3]) # RIGHT action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The action leading to the final state has been correctly updated in Q. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *mean* and *max* operator over the actions to compute an average and a max version of a state-value matrix from the action-state matrix (i.e.: average/max over the action). Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean State Values (averaging over actions):\n",
      "[[ 1.30479029e-03  3.62738352e-02 -1.12312094e-02  9.25242773e-04]\n",
      " [ 7.01739929e-03  5.96090276e-04  2.55581571e-02  4.81290890e-02]\n",
      " [ 1.61031730e-02  1.40143767e-03  1.20584767e-02  2.58119142e+00]\n",
      " [-5.68793300e-02 -2.67220641e-02  5.48452351e-02  5.75304335e-02]]\n",
      "\n",
      "Max State Values (maximum over actions):\n",
      "[[ 0.04543105  0.07344787  0.06429783  0.04137334]\n",
      " [ 0.03121148  0.02593439  0.05854883  0.09185315]\n",
      " [ 0.0742916   0.04372309  0.03413102 10.0883503 ]\n",
      " [ 0.02072496  0.02748921  0.09050557  0.07180548]]\n"
     ]
    }
   ],
   "source": [
    "V_mean = np.mean(Q,axis=2)\n",
    "V_max = np.max(Q,axis=2)\n",
    "\n",
    "print(\"Mean State Values (averaging over actions):\")\n",
    "print(V_mean)\n",
    "print(\"\\nMax State Values (maximum over actions):\")\n",
    "print(V_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q-learning for $10$ more episodes (without resetting your action-value matrix!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "☺ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ ☺ □ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ ☺ □\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "□ □ □ Ω\n",
      "\n",
      "\n",
      "Α □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ □\n",
      "□ □ □ ☺\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    state, reward, termination , msgs = W.reset()\n",
    "    while not termination:\n",
    "        action = behaviour_policy(state,Q,epsilon)\n",
    "        next_state,reward, termination, msgs = W.step(action)\n",
    "\n",
    "        current_q = Q[state.x, state.y, action]\n",
    "        next_max_q = np.max(Q[next_state.x, next_state.y,:]) if not termination else 0\n",
    "        Q[state.x, state.y,action] = current_q + alpha * (reward + gamma * next_max_q - current_q)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "        W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print again from the action-state matrix, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for state (2,3):\n",
      "UP: 1.1018280251295596\n",
      "DOWN: 1.525047396746606\n",
      "LEFT: 68.64974623506725\n",
      "RIGHT: 4.504982503269069\n",
      "\n",
      "Q-values for state (3,2):\n",
      "UP: 0.01981638408356259\n",
      "DOWN: 0.03659605625347511\n",
      "LEFT: 0.07246293084005005\n",
      "RIGHT: 0.09050556937512302\n"
     ]
    }
   ],
   "source": [
    "# Print Q-values for state (2,3)\n",
    "print(\"Q-values for state (2,3):\")\n",
    "print(\"UP:\", Q[2,3,0])    # UP action\n",
    "print(\"DOWN:\", Q[2,3,1])  # DOWN action\n",
    "print(\"LEFT:\", Q[2,3,2])  # LEFT action\n",
    "print(\"RIGHT:\", Q[2,3,3]) # RIGHT action\n",
    "\n",
    "print(\"\\nQ-values for state (3,2):\")\n",
    "print(\"UP:\", Q[3,2,0])    # UP action\n",
    "print(\"DOWN:\", Q[3,2,1])  # DOWN action\n",
    "print(\"LEFT:\", Q[3,2,2])  # LEFT action\n",
    "print(\"RIGHT:\", Q[3,2,3]) # RIGHT action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We now notice more clearly a gradient towards the final state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider how learning would change if you were to change the learning parameter, or if the environment were to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Alpha may change the speed of convergence, epsilon may change the degree of exloration. Even if the envoroment where to change, the agent would keep exploring it following its behaviour policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximators\n",
    "So far we have solved the reinforcement learning problem using tables and matrices to store state-value, $v$, functions and action-value, $q$, functions. This is called the **tabular** approach to RL. This solution is simple and interpretable, but it has a severe limitation.\n",
    "\n",
    "What happens if our gridworld becomes much bigger, extending to a 10000 by 10000 grid? How much memory would we need to store the state-value or the action-value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A lot of memory. 1000^2 * 8bytes = 0.8gb. For the action value function we need 4 times more 0.8*4gb=3.2gb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you imagine a way to solve this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We could use a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we studied the basics of **reinforcement learning** through the analysis on a simple *gridworld* environment of two core RL problems: **policy evaluation** and **policy improvement**.\n",
    "\n",
    "We first considered the problem of policy evaluation, that is, the problem of assessing how good a policy is. For **deterministic policies** we solved the problem simply by computing the *total discounted return* of a policy. After realizing the limitation of deterministic policies, we turned to **stochastic policies**, and we computed their *return in expectation* relying on *Monte Carlo methods*.\n",
    "\n",
    "Next we considered the problem of policy improvement, that is, the problem of learning optimal policies. For this task, we realized that a global value for a policy is not handy when improving a policy, and, instead local values would be more useful. We then computed **state values** and **action values** to assess a policy step by step, and used this values to take improved actions. This iterative approach, based on evaluating a policy and improving the policy, is an instance of the more generic method called **Generalized Policy Iteration**.\n",
    "\n",
    "In the final optional part, we considered a more advanced RL algorithm, **Q-learning**. Q-learning allowed us to learn an optimal policy while interacting with an environment; this algorithm had two important features: (i) it learns in a continuous way, step-by-step, and not on complete episodes; (ii) it learns an optimal policy while behaving according to a different, more exploration-prone, policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in3050",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
