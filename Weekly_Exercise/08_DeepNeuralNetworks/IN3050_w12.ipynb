{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3050/4050 - Week 12: Deep Learning Glimpse\n",
    "\n",
    "## Deep Learning\n",
    "Deep learning is a big topic, and we can't hope to cover it in any depth here. But, as an introduction we are going to take a look at some state of the art tools used in both research and industry: especially PyTorch.\n",
    "\n",
    "We are also going to take a look at one of the biggest problems with deep neural models, the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports used in the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1) Introduction to PyTorch\n",
    "[PyTorch](https://pytorch.org/) is a machine learning framework designed specifically for deep learning, but can be used for all sorts of computations. PyTorch uses highly optimized and accelerated code for most of the operations needed to make state of the art neural networks.\n",
    "\n",
    "In this course we will only be using PyTorch but there exists many other deep learning frameworks, e.g. [TensorFlow](https://www.tensorflow.org/), [Flax](https://flax.readthedocs.io/en/latest/), [Caffe](https://caffe.berkeleyvision.org/), and many more.\n",
    "\n",
    "This [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) is a useful reference while completing this exercise.\n",
    "\n",
    "**Note:** We use PyTorch 2.2 for these exercises, but earlier versions should also be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple operations/tensors\n",
    "\n",
    "#### Tensors\n",
    "In PyTorch, the basic object is a tensor. Tensors are specialized data structures very similar to vectors and matrices, and, consequently to Numpy arrays. Tensors are used to encode the inputs and outputs of models, as well as the models' parameters. A tensor can be made from a Numpy array, and can be cast to a Numpy array. Importantly, Pytorch tensors are optimized for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 3])\n",
      "\n",
      "Converted to array:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = np.arange(1, 10).reshape((3, 3))\n",
    "x = torch.tensor(data)\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(type(x))\n",
    "print(x.shape)\n",
    "\n",
    "print(\"\\nConverted to array:\")\n",
    "print(x.numpy())\n",
    "print(type(x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations\n",
    "PyTorch also comes with all of the standard math operations. Again, these look very similar to the ones found in Numpy. At this point you might wonder why use tensors when they are so similar to Numpy arrays. Later in this exercise we are going to use some of the cool features of PyTorch tensors that Numpy arrays don't have.\n",
    "\n",
    "#### Initialization\n",
    "At the beginning of the trainining, we often need to randomly initialize the model weights. For example, one can use uniform random values from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1771, 0.7748, 0.9761, 0.3730, 0.9107, 0.4489],\n",
       "        [0.3097, 0.6188, 0.6389, 0.4490, 0.5316, 0.3862],\n",
       "        [0.5382, 0.0049, 0.5906, 0.5828, 0.5695, 0.2024],\n",
       "        [0.3704, 0.5323, 0.5422, 0.8575, 0.1284, 0.2461]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(4, 6)\n",
    "random_weights = nn.init.uniform_(x, a=0.0, b=1.0) # Random uniform initializer from PyTorch\n",
    "random_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1)\n",
    "Use PyTorch tensors to do the following:\n",
    "\n",
    "1. Create two tensors `x` and `y` with the values `3` and `7`.\n",
    "2. Multiply the values and assign the result to `z`.\n",
    "3. Create a matrix `A` with the shape $3 \\times 3$ and a column vector `b` with 3 elements using the random uniform initializer with values from $-1$ to $1$.\n",
    "4. Multiply the matrix and the vector together and assign the result to `c`.\n",
    "5. Create a Numpy column vector of shape (3, 1) with the values [1, 2, 3] and add these elementwise to `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5422],\n",
      "        [-0.9832],\n",
      "        [ 0.2051]])\n",
      "tensor([[0.4578],\n",
      "        [1.0168],\n",
      "        [3.2051]], dtype=torch.float64)\n",
      "tensor([[0.4578],\n",
      "        [1.0168],\n",
      "        [3.2051]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kd/j0t_qpxn5232l1_zzhlcb2wc0000gn/T/ipykernel_52102/1210609160.py:19: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  c = c + d\n"
     ]
    }
   ],
   "source": [
    "# 1)\n",
    "x = torch.tensor(3)\n",
    "y = torch.tensor(7)\n",
    "\n",
    "# 2)\n",
    "z = x * y\n",
    "\n",
    "# 3)\n",
    "A = nn.init.uniform_(torch.empty(3, 3), a=-1, b=1)\n",
    "b = nn.init.uniform_(torch.empty(3, 1), a=-1, b=1)\n",
    "\n",
    "# 4)\n",
    "c = A @ b  # equivalent to c = A.matmul(b)\n",
    "print(c)\n",
    "\n",
    "# 5) Update b with new values:\n",
    "d = np.array([[1], [2], [3]])\n",
    "\n",
    "c = c + d\n",
    "print(c)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2) Activation function\n",
    "Implement the sigmoid activation function and its derivative using PyTorch. The exponential function is available as `torch.exp(x)`.\n",
    "\n",
    "Activation:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Gradient:\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Return sigmoid activation of x.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Tensor to calculate activations for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : torch.Tensor\n",
    "        The activations.\n",
    "    \"\"\"\n",
    "    a = 1 / (1 + torch.exp(-x))\n",
    "    return a\n",
    "\n",
    "def my_sigmoid_grad(a):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the sigmoid function from the value of the activation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.Tensor\n",
    "        Output from my_sigmoid().\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : torch.Tensor\n",
    "        Gradient of the sigmoid function.\n",
    "    \"\"\"\n",
    "    grad = a * (1 - a)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course plot the values in PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Gradient')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(np.linspace(-10, 10, 100))\n",
    "a = my_sigmoid(x)\n",
    "grad = my_sigmoid_grad(a)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(x, a); ax[0].set_title(\"Activation\")\n",
    "ax[1].plot(x, grad); ax[1].set_title(\"Gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic gradient\n",
    "PyTorch comes with its own automatic differentiation engine called [Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html). Using Autograd, one can get the gradient of a sequence of PyTorch operations without doing any manual differentiation.\n",
    "\n",
    "#### Exercise 1.3)\n",
    "Evaluate your function `my_sigmoid()` in all the points of the tensor `x` in the cell below, and use PyTorch automatic differentiation to calculate the gradient of the function in these points. Compare to your own implementation of the gradient.\n",
    "\n",
    "Check the link to the autograd tutorial above, but the most important points to remember are:\n",
    "1. You should explicitly tell PyTorch to track gradients for your tensor (`requires_grad=True`)\n",
    "2. In Pytorch, the actual computing of the gradients happens when you do the backward pass by calling `.backward()` on the last output of your computations. In your case, it will be the original tensor transformed by the sigmoid function. However, the default assumption of PyTorch is that the backward pass starts from the final *loss* value which is scalar - but your final data point is a vector. To avoid errors, simply convert the final tensor into a scalar before running the backward pass: for example, by applying `.sum()` on it.\n",
    "3. After running the backward pass, the gradient values can be retrieved from the `.grad` property of the respective tensor.\n",
    "4. Matplotlib can't plot PyTorch tensors directly, convert them to Numpy arrays first (`.detach().numpy()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15a861790>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradients from your own function:\n",
    "a = my_sigmoid(x)\n",
    "gradients = my_sigmoid_grad(a)\n",
    "\n",
    "# Gradients from PyTorch:\n",
    "x2 = torch.tensor(np.linspace(-10, 10, 100), requires_grad=True)  # Telling PyTorch to track gradients for the tensor\n",
    "a2 = my_sigmoid(x2)\n",
    "a2.sum().backward()  # Doing the backward pass (i.e., computing gradients)\n",
    "pytorch_gradients = x2.grad.detach().numpy()  # .detach().numpy() is to pass the values to Matplotlib\n",
    "\n",
    "# Plot of the gradients from my_sigmoid_grad() and Pytorch.\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, pytorch_gradients, label=\"Using PyTorch\")\n",
    "ax.plot(x, gradients, '.', label=\"my_sigmoid_grad\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything correctly the dots should follow the line precisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "Pytorch comes with many implementations of functions useful to deep learning. The `torch.nn` module provides all the blocks you need to build your own neural network. It is a higher lever API that allows you to very quickly and easily create complex models without needing to worry about all the minor details.\n",
    "\n",
    "\n",
    "#### Sequential feedforward neural network in PyTorch\n",
    "\n",
    "There are very few limits on what you can do with PyTorch, but to keep it simple we will restrict our models to \"sequential\" models. This means that the layers are all stacked on top each other and there are no loops, skip connections or forks. One layer feeds in to the next. These types of models are very easy to define using PyTorch.\n",
    "\n",
    "Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers).\n",
    "\n",
    "Below we define a simple network with an input layer of size 4,  two hidden layers of size 2 and a output layer of only one node. All the layers use a simple sigmoid activation function except the last one, where we skip the activation. Notice that we explicitly specify a simple random uniform initializer for the weights of our linear layers (by default PyTorch will use a more advanced initializer, but we want to keep it simple for now).\n",
    "\n",
    "![Model structure](figures/4_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_sigmoid_stack): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=2, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 19\n",
      "<class '__main__.NeuralNetwork'>\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_sigmoid_stack = nn.Sequential(\n",
    "            nn.Linear(4, 2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(2, 2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(2, 1),\n",
    "        )\n",
    "       \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_sigmoid_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Initialization: first check the layer type,\n",
    "# then apply the desired changes to the weights\n",
    "def init_uniform(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.uniform_(layer.weight)\n",
    "    \n",
    "model = NeuralNetwork()\n",
    "model.apply(init_uniform)\n",
    "print(model)\n",
    "nr_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {nr_params}\")\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.4)\n",
    "Use PyTorch to define a class and function that returns a sequential neural model. The model should have input size of 2, three hidden layers with two nodes and one output layer with two nodes. All layers except the last layer should apply the activation function specified in the function argument. Use the random uniform initializer with values from $-1$ to $1$.\n",
    "\n",
    "![Model structure](figures/2_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        if not kwargs[\"activation_function\"]:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = kwargs[\"activation_function\"]\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            self.activation,\n",
    "            nn.Linear(2, 2),\n",
    "            self.activation,\n",
    "            nn.Linear(2, 2),\n",
    "            self.activation,\n",
    "            nn.Linear(2, 2),\n",
    "        )\n",
    "       \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "def build_model(activation=nn.Sigmoid()):\n",
    "    \"\"\"\n",
    "    Return a sequential model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : function\n",
    "        Function from PyTorch activations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : PyTorch Sequential model.\n",
    "    \"\"\"\n",
    "    # Initialization: first check the layer type,\n",
    "    # then apply the desired changes to the weights\n",
    "    def init_uniform(layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            nn.init.uniform_(layer.weight, a=-1, b=1)\n",
    "\n",
    "    model = NeuralNetwork(activation_function=activation)\n",
    "    model.apply(init_uniform)\n",
    "    print(model)\n",
    "    nr_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters in the model: {nr_params}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): Sigmoid()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): Sigmoid()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n",
      "NeuralNetwork(\n",
      "  (activation): Sigmoid()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): Sigmoid()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n"
     ]
    }
   ],
   "source": [
    "# Crate model with sigmoid activation\n",
    "model = build_model(activation=nn.Sigmoid())\n",
    "print(model)\n",
    "\n",
    "# Print number of parameters\n",
    "nr_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {nr_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2) Vanishing Gradients\n",
    "In this section we are going to look at a two different ways to cope with the vanishing gradient problem.\n",
    "\n",
    "- Change of activation function\n",
    "- \"Proper\" initialization of weights\n",
    "\n",
    "There are other techniques that can have a big impact on this problem, but these are the simplest and easiest to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "The sigmoid activation function you implemented above\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "has two properties that make it good for neural networks.\n",
    "- It's differentiable everywhere.\n",
    "- Its gradient has a very simple form based on the output of the function.\n",
    "\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "However, there are some issues with this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1)\n",
    "Can you name and explain two problems with this activation function?\n",
    "\n",
    "**Hint:** what happens when you evaluate the function for high or low values of $x$? And what is the domain of the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "**Vanishing Gradient Problem:** For very large  (positive or negative) input values, the sigmoid function saturated at 0 or 1, causing its gradient to become neraly zero. This maked learning difficult because the weights barly update druing backpropagation. \n",
    "\n",
    "**Non-Zero Contered Output:** The sigmoid function outputs values betwen [0,1], meaning its not zero-centered. This can create a bias in the larning process since all valsue positve, leadning to less effiecient traning as gradients only maove in one direction. \n",
    "\n",
    "This is why NN often use ReLU or tanh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2)\n",
    "Use the function `build_model()` to create a model using the sigmoid activation function.\n",
    "\n",
    "In the next code block, we run one iteration over a training set and make a \"violin\" plot of the gradients (the implementation is imported from an external file, feel free to have a look at it). \n",
    "\n",
    "What do you see? Give a short description of the plot. Are the results suprising?\n",
    "\n",
    "You can use your own implementation of the sigmoid from earlier, or you can use PyTorch sigmoid implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): Sigmoid()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): Sigmoid()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n"
     ]
    }
   ],
   "source": [
    "model_sig = build_model(activation=nn.Sigmoid())\n",
    "\n",
    "from helpers12 import plot_grad_pytorch as plot_grad\n",
    "fig = plot_grad(model_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The size of the gradient goes towards zero as we move back through the layers. Should not be suprising as we are using a naive initialization and a saturating activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better activation functions, tanh\n",
    "#### Exercise 2.3) Can we solve the problems by using the tanh activation function? Why? Why not?\n",
    "\n",
    "**Answer:** NO plot. TODO look at soluton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.4)\n",
    "Create a model using the tanh activation function from PyTorch and plot the gradients again. Comment on the result, and how it differs from the sigmoid case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): Tanh()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n"
     ]
    }
   ],
   "source": [
    "model_tanh = build_model(activation=nn.Tanh())\n",
    "\n",
    "fig = plot_grad(model_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The tanh function is zero centered giving outputs that can be negative. This opens up directions in our optimization space not available if we use the sigmoid function.\n",
    "\n",
    "The tanh function still saturates at the extremes, so we still expect to see the gradients go towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better activation functions, ReLU\n",
    "The last activation function we are going to take a look at is the Rectified Linear Unit (ReLU). This function is very different from the sigmoid and the tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "activation = nn.ReLU()\n",
    "a = activation(torch.Tensor(x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, a)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is defined as\n",
    "\n",
    "$$    f(x)= \n",
    "\\begin{cases}\n",
    "    0, & \\text{if } x < 0\\\\\n",
    "    x, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### Exercise 2.5)\n",
    "Can this activation function solve some of our problems?\n",
    "\n",
    "**Answer**: ReLU helps mitigate the vanishing gradient problem, provides computational efficiency, and allows for faster convergence during traning. I has an issue with \"dying ReLU\" problem, where neurons can become inactive and stop learning if they consistenly output zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.6)\n",
    "As before, build a new model with this activation function and comment on the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): ReLU()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n"
     ]
    }
   ],
   "source": [
    "model_relu = build_model(activation=nn.ReLU())\n",
    "                    \n",
    "fig = plot_grad(model_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The gradients do not vanish, but are clustered close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dead neurons\n",
    "The ReLU activation function has the problem that the sigmoid and tanh functions do not have. During training, we can end up in a situation where the input to the activation function in a node is always less than zero. In this case, the gradient going back through that node will ALWAYS be zero, i.e the node \"dies\" and does no longer take part in training.\n",
    "\n",
    "Other activation functions have been proposed to deal with this, from the simple Leaky-ReLU to the more interesting SELU. We will not explore these here, but when you are training your own models they are worth taking a look at. They are all implemented in PyTorch, see https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights\n",
    "Before we start training a neural network, we must initialize the weights and biases of the network. There are several ways we can do this.\n",
    "\n",
    "#### Exercise 2.7) Random uniform initialization\n",
    "Imagine you are going to train a network with *many* nodes in the hidden layers using $\\tanh$ as an activation function. You decide to initialize the weights of the network using a random uniform distribution in the range [-1, 1]. Can you think of any issues with this approach?\n",
    "\n",
    "**Hint:** What happens to the sum inside the activation function as the number of neurons in the previous layer increase?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "**1.Saturation:** As the number of neurons increases, the sum of the inputs to the Than function can become large, causing the outputs to saturate near -1 or 1. This results in very small gradiant during backpropagation, leading to slow learning or the vanishing gradient problem.\n",
    "\n",
    "**Inefficient Weigts Updates**:When many neurosn output values close to the saturation point, the gradient become neglible, making it difficult for the netwrok to learn effecticly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glorot normal initialization\n",
    "Glorot initialization is a scheme where the size of the intitial weights depends on the number of neurons/weights in a layer. In this variant, the weights are initialized as samples from a normal distribution.\n",
    "\n",
    "$$W_l \\sim \\mathcal{N}(\\mu=0, \\sigma_l)$$\n",
    "\n",
    "And $\\sigma_l$ is on the form\n",
    "\n",
    "$$\\sigma_l = \\sqrt{\\frac{2}{n_l + n_{l+1}}}$$\n",
    "\n",
    "Where $n_i$ is the number of neurons in layer $i$. The weight matrix $W_l$ is of size $n_l\\times n_{l+1}$.\n",
    "\n",
    "As the number of neurons and weights **increases**, the \"range\" of the initial values **decreases** so that the activations are likely to stay in the center range of the activation function where the gradient is large. The expression for $\\sigma$ has a theoretical underpinning that we won't look at here, but if interested you can check out the [paper by Xavier Glorot and Yoshua Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi). In it, they use a uniform distribution as an example. But the results are general for any kind of (reasonable) distribution.\n",
    "\n",
    "In PyTorch, this initialization method is named `nn.init.xavier_normal_()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.8)\n",
    "Define a new function `build_model_glorot()`, that uses Glorot normal initialization in the layers of the model. You can copy and adapt the function you implemented in exercise 1.4.\n",
    "\n",
    "The docs for the PyTorch Glorot normal initializer can be found [here](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_glorot(activation=nn.Sigmoid()):\n",
    "    \"\"\"\n",
    "    Return a sequential model with Glorot initialization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : function\n",
    "        Function from PyTorch activations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : PyTorch Sequential model.\n",
    "    \"\"\"\n",
    "    # Initialization: first check the layer type,\n",
    "    # then apply the desired changes to the weights\n",
    "    def init_glorot(layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "    model = NeuralNetwork(activation_function=activation)\n",
    "    model.apply(init_glorot)\n",
    "    print(model)\n",
    "    nr_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters in the model: {nr_params}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients with better initialization\n",
    "\n",
    "#### Sigmoid\n",
    "Let's take a look at the gradients when we are using a sigmoid activation function and the Glorot normal intiialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): Sigmoid()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): Sigmoid()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n"
     ]
    }
   ],
   "source": [
    "model2_sig = build_model_glorot(activation=nn.Sigmoid())\n",
    "                    \n",
    "fig = plot_grad(model2_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the figure from exercise 2.2, Notice the scale on the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We can see a marked improvement in the backpropagation of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh\n",
    "Now compare the tanh activation function model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): Tanh()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n"
     ]
    }
   ],
   "source": [
    "model2_tanh = build_model_glorot(activation=nn.Tanh())\n",
    "                    \n",
    "fig = plot_grad(model2_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The scale of the activations has increased, And the gradients do not seem to vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n",
    "Finally, compare a new ReLU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): ReLU()\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters in the model: 24\n"
     ]
    }
   ],
   "source": [
    "model2_relu = build_model_glorot(activation=nn.ReLU())\n",
    "                    \n",
    "fig = plot_grad(model2_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:**\n",
    "The scale of the gradients has increased a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Frameworks\n",
    "In this exercise we have taken a look at the PyTorch framework. Such frameworks take a lot of pain out of creating complex deep learning models, and allow for quick development of efficient code. \n",
    "\n",
    "### Vanishing Gradient\n",
    "Two ways to reduce the problem of vanishing gradients are to use a suitable activation function and an initialization scheme.\n",
    "\n",
    "The sigmoid (logistic) function has a few problems that we can improve upon with other activation functions, and a proper initialization of the weights can make a huge difference.\n",
    "\n",
    "We have only scratched the surface of possible activation functions and initializers. And there are also many other techniques that are employed to speed up training of neural networks, but these are left for later courses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in3050",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
